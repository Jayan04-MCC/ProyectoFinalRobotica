# Proyecto Final de RobÃ³tica - Deep Q-Learning con CUDA

Este proyecto implementa **Deep Q-Network (DQN)** acelerado con CUDA para resolver problemas de navegaciÃ³n y control robÃ³tico. El proyecto estÃ¡ dividido en tres componentes principales que demuestran la escalabilidad del algoritmo desde entornos simulados simples hasta implementaciones en hardware real.

## Integrantes del Proyecto

| Integrante | ParticipaciÃ³n |
|-----------|---------------|
| DIAZ CASTRO, BERLY JOEL | 14.28% |
| MARIÃ‘OS HILARIO, PRINCCE YORWIN | 14.28% |
| YANQUI VERA, HENRY ARON | 14.28% |
| CACERES CUBA, JAYAN MICHAEL | 14.28% |
| APAZA CONDORI, JHON ANTHONY | 14.28% |
| ARONI JARATA, ANTONY | 14.28% |
| CARAZAS QUISPE, ALESSANDER JESUS | 14.28% |

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n General](#-descripciÃ³n-general)
- [Componentes del Proyecto](#-componentes-del-proyecto)
  - [1. GridWorld 10x10](#1-gridworld-10x10)
  - [2. GridWorld 20x20](#2-gridworld-20x20)
  - [3. ImplementaciÃ³n en Webots](#3-implementaciÃ³n-en-webots)
- [Requisitos del Sistema](#-requisitos-del-sistema)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso](#-uso)
- [Arquitectura TÃ©cnica](#-arquitectura-tÃ©cnica)
- [Resultados](#-resultados)
- [Autores](#-autores)

## ğŸ¯ DescripciÃ³n General

Este proyecto demuestra la aplicaciÃ³n de **Deep Reinforcement Learning** utilizando **DQN (Deep Q-Network)** con aceleraciÃ³n CUDA para resolver problemas de navegaciÃ³n autÃ³noma. La implementaciÃ³n progresa desde entornos de cuadrÃ­cula simples hasta un robot fÃ­sico E-puck en el simulador Webots.

### CaracterÃ­sticas Principales

- âœ… **AceleraciÃ³n CUDA**: Entrenamiento GPU-optimizado usando Unified Memory
- âœ… **Double DQN**: Reduce sobreestimaciÃ³n de valores Q
- âœ… **Experience Replay**: Buffer de experiencias para estabilidad del entrenamiento
- âœ… **Target Network**: Red objetivo para convergencia estable
- âœ… **Epsilon-Greedy**: Estrategia de exploraciÃ³n adaptativa
- âœ… **Gradient Clipping**: PrevenciÃ³n de explosiÃ³n de gradientes
- âœ… **ComunicaciÃ³n Socket**: IntegraciÃ³n Jetson-PC para entrenamiento distribuido

## ğŸ§© Componentes del Proyecto

### 1. GridWorld 10x10

**UbicaciÃ³n**: `grid10/`

ImplementaciÃ³n de DQN en un entorno de cuadrÃ­cula 10x10 con obstÃ¡culos. El agente debe navegar desde la posiciÃ³n inicial (0,0) hasta la meta (9,9) evitando obstÃ¡culos.

#### CaracterÃ­sticas
- **TamaÃ±o de cuadrÃ­cula**: 10x10 (100 estados)
- **Acciones**: 4 (arriba, abajo, izquierda, derecha)
- **Red neuronal**: 100 â†’ 128 â†’ 4
- **Batch size**: 64
- **Replay buffer**: 10,000 experiencias
- **Episodios mÃ¡ximos**: 800

#### CompilaciÃ³n
```bash
cd grid10
nvcc -o dqn_jetson_fixed dqn_jetson_fixed.cu -O3 -arch=sm_72
```

> **Nota**: Ajusta `-arch=sm_72` segÃºn tu GPU (sm_72 para Jetson Xavier, sm_75 para RTX 2080, etc.)

#### EjecuciÃ³n
```bash
./dqn_jetson_fixed
```

#### Recompensas
- **Meta alcanzada**: +10.0
- **Acercamiento**: +1.0
- **Alejamiento**: -2.0
- **Sin movimiento**: -0.5
- **ColisiÃ³n**: -3.0

![GridWorld 10x10 - Entrenamiento](img/001-gridworld-10x10.png)
![GridWorld 10x10 - Q-values](img/002-gridworld-10x10.png)
![GridWorld 10x10 - PolÃ­tica aprendida](img/003-gridworld-10x10.png)

---

### 2. GridWorld 20x20

**UbicaciÃ³n**: `grid20/`

VersiÃ³n escalada del GridWorld con mayor complejidad y capacidad de red neuronal aumentada.

#### CaracterÃ­sticas
- **TamaÃ±o de cuadrÃ­cula**: 20x20 (400 estados)
- **Acciones**: 4 (arriba, abajo, izquierda, derecha)
- **Red neuronal**: 400 â†’ 1024 â†’ 4
- **Batch size**: 256
- **Replay buffer**: 150,000 experiencias
- **Episodios mÃ¡ximos**: 5,000
- **Pasos mÃ¡ximos por episodio**: 250

#### CompilaciÃ³n
```bash
cd grid20
nvcc -o dqn_jetson_fixed_moreparams dqn_jetson_fixed_moreparams.cu -O3 -arch=sm_72
```

#### EjecuciÃ³n
```bash
./dqn_jetson_fixed_moreparams
```

#### Diferencias con 10x10
- **Mayor capacidad de red**: 1024 neuronas ocultas vs 128
- **Replay buffer mÃ¡s grande**: 150K vs 10K experiencias
- **Learning rate mÃ¡s bajo**: 0.0001 vs 0.001
- **Mayor nÃºmero de episodios**: 5000 vs 800

---

### 3. ImplementaciÃ³n en Webots

**UbicaciÃ³n**: `weboots_sockets/`

ImplementaciÃ³n de DQN para un robot E-puck que sigue lÃ­neas negras en el simulador Webots. Utiliza comunicaciÃ³n por sockets entre:
- **Jetson AGX Xavier**: Ejecuta el entrenamiento DQN con CUDA
- **PC**: Ejecuta el simulador Webots y el controlador del robot

#### Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         Socket TCP          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jetson AGX Xavier â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    PC (Webots)       â”‚
â”‚                     â”‚      Puerto 5000            â”‚                      â”‚
â”‚  - DQN Training     â”‚                             â”‚  - SimulaciÃ³n        â”‚
â”‚  - CUDA Kernels     â”‚   Estado (sensores)         â”‚  - E-puck Controller â”‚
â”‚  - Policy Network   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º        â”‚  - Entorno           â”‚
â”‚  - Target Network   â”‚                             â”‚                      â”‚
â”‚  - Replay Buffer    â”‚   AcciÃ³n (motores)          â”‚                      â”‚
â”‚                     â”‚   â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![Arquitectura Webots-Socket](img/webots-socket.png)

#### CaracterÃ­sticas
- **Estado**: 6 dimensiones
  - 3 sensores de suelo (izquierdo, centro, derecho)
  - Error lateral
  - 2 velocidades de ruedas
- **Acciones**: 3 (girar izquierda, recto, girar derecha)
- **Red neuronal**: 6 â†’ 256 â†’ 3
- **Batch size**: 64
- **Replay buffer**: 50,000 experiencias

#### CompilaciÃ³n

**En Jetson (Entrenador DQN)**:
```bash
cd weboots_sockets
nvcc -o trainer main.cu -O3 -arch=sm_72 -std=c++11
```

**En PC (Controlador Webots)**:
El controlador Python se encuentra en `weboots_sockets/e-puck/controllers/`

#### EjecuciÃ³n

**Paso 1: Iniciar el entrenador en Jetson**
```bash
./trainer
# EsperarÃ¡ conexiÃ³n en puerto 5000
```

**Paso 2: Abrir Webots en PC**
```bash
# Abrir el mundo en: weboots_sockets/e-puck/worlds/
# El controlador se conectarÃ¡ automÃ¡ticamente al Jetson
```

#### FunciÃ³n de Recompensa
```
Recompensa = +2.0 (centro sobre lÃ­nea)
           + 1.5 Ã— balance_simÃ©trico
           - 0.5 Ã— error_lateral
           + 1.0 Ã— velocidad_promedio
           - 0.2 Ã— diferencia_velocidades
           - 10.0 (pÃ©rdida total de lÃ­nea)
           + 3.0 (estado ideal: todos sobre negro)
```

#### Archivos Principales
- `main.cu`: Entrenador DQN con servidor socket
- `config.h`: HiperparÃ¡metros del sistema
- `cuda_kernels.cuh`: Kernels CUDA para forward/backward pass
- `dqn_agent.h`: Definiciones del agente DQN
- `types.h`: Estructuras de datos

![E-puck Robot](img/e-puck1.png)
![E-puck en Webots](img/e-puck-2.png)

---

## ğŸ’» Requisitos del Sistema

### Hardware
- **GPU NVIDIA** con soporte CUDA (Compute Capability â‰¥ 7.0)
  - Jetson AGX Xavier (sm_72)
  - RTX 2080/3080 (sm_75/sm_86)
  - O superior
- **RAM**: MÃ­nimo 4GB (8GB recomendado para grid20)

### Software
- **CUDA Toolkit**: 10.2 o superior
- **nvcc**: Compilador CUDA
- **GCC/G++**: 7.5 o superior
- **Webots**: R2023a o superior (solo para componente 3)
- **Python**: 3.8+ con librerÃ­as:
  - `controller` (mÃ³dulo de Webots)
  - `socket`

### Sistema Operativo
- Ubuntu 18.04/20.04/22.04
- Jetson Linux (L4T) para Jetson Xavier

## ğŸ”§ InstalaciÃ³n

### 1. Instalar CUDA Toolkit

**Ubuntu/PC**:
```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"
sudo apt-get update
sudo apt-get -y install cuda
```

**Jetson Xavier**:
```bash
# CUDA viene preinstalado con JetPack
sudo apt-get update
sudo apt-get install nvidia-jetpack
```

### 2. Verificar InstalaciÃ³n CUDA
```bash
nvcc --version
nvidia-smi  # Solo en PC, no disponible en Jetson
```

### 3. Clonar el Repositorio
```bash
git clone https://github.com/Jayan04-MCC/ProyectoFinalRobotica.git
cd ProyectoFinalRobotica
```

### 4. Instalar Webots (Opcional, solo para componente 3)
```bash
# Descargar desde: https://cyberbotics.com/
# O usar snap:
sudo snap install webots
```

## ğŸš€ Uso

### Entrenamiento GridWorld 10x10
```bash
cd grid10
nvcc -o dqn_jetson_fixed dqn_jetson_fixed.cu -O3 -arch=sm_72
./dqn_jetson_fixed
```

**Salida esperada**:
```
=== Deep Q-Learning con CUDA Unified Memory - VERSIÃ“N CORREGIDA ===
Grid: 10x10 | Hidden: 128 | Batch: 64
LR: 0.0010 -> 0.0001 (decay: 0.99990) | Gamma: 0.95
Epsilon: 1.00 -> 0.10 (decay: 0.9990)
Target update: 1000 steps | Grad clip: 5.0 | TD clip: 5.0

Using GPU: NVIDIA Tegra Xavier
Compute Capability: 7.2
Unified Memory: Soportada
Concurrent Managed Access: SÃ­ (Ã“ptimo para Jetson)

Llenando replay buffer (target: 1000 experiencias)...
Buffer: 1000 experiencias en 45 episodios
Ã‰xitos en buffer inicial: 12 (1.2%)

Iniciando entrenamiento...
Ep  100 | AvgR:  -8.45 | Steps:  42.3 | Eps: 0.905 | LR: 0.00099 | Last100: 15.0%
Ep  200 | AvgR:  -2.31 | Steps:  28.7 | Eps: 0.819 | LR: 0.00098 | Last100: 45.0%
...
*** Â¡Convergencia alcanzada en episodio 456! ***
```

### Entrenamiento GridWorld 20x20
```bash
cd grid20
nvcc -o dqn_jetson_fixed_moreparams dqn_jetson_fixed_moreparams.cu -O3 -arch=sm_72
./dqn_jetson_fixed_moreparams
```

### Entrenamiento con Webots

**Terminal 1 (Jetson)**:
```bash
cd weboots_sockets
nvcc -o trainer main.cu -O3 -arch=sm_72 -std=c++11
./trainer
# Esperando conexiÃ³n...
```

**Terminal 2 (PC con Webots)**:
```bash
# 1. Abrir Webots
# 2. File â†’ Open World â†’ Navegar a weboots_sockets/e-puck/worlds/
# 3. Seleccionar el archivo .wbt
# 4. Play â–¶
```

El controlador Python se conectarÃ¡ automÃ¡ticamente al Jetson y comenzarÃ¡ el entrenamiento.

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Red Neuronal DQN

**Arquitectura**:
```
Input Layer (Estado)
    â†“
Dense Layer (ReLU)
    â†“
Output Layer (Q-values por acciÃ³n)
```

**ParÃ¡metros por componente**:

| Componente | Input | Hidden | Output | ParÃ¡metros |
|-----------|-------|--------|--------|------------|
| Grid 10x10 | 100 | 128 | 4 | ~13K |
| Grid 20x20 | 400 | 1024 | 4 | ~414K |
| Webots | 6 | 256 | 3 | ~2.3K |

### Algoritmo DQN

1. **InicializaciÃ³n**:
   - Red policy (Î¸) con pesos aleatorios
   - Red target (Î¸â») = copia de Î¸
   - Replay buffer vacÃ­o

2. **Loop de entrenamiento**:
   ```
   Para cada episodio:
     Observar estado s
     Para cada paso:
       a = Îµ-greedy(s, Î¸)
       Ejecutar a, observar r, s'
       Guardar (s, a, r, s') en buffer
       
       Si es momento de entrenar:
         Muestrear batch del buffer
         Calcular TD-target: y = r + Î³Â·max_a' Q(s', a'; Î¸â»)
         Actualizar Î¸ minimizando (Q(s,a;Î¸) - y)Â²
       
       Si es momento de actualizar target:
         Î¸â» â† Î¸
   ```

3. **Double DQN**:
   - Policy network selecciona mejor acciÃ³n: a* = argmax_a Q(s', a; Î¸)
   - Target network evalÃºa esa acciÃ³n: y = r + Î³Â·Q(s', a*; Î¸â»)

### Kernels CUDA

**Forward Pass**:
- `forward_hidden_kernel`: Capa oculta con ReLU
- `forward_output_kernel`: Capa de salida (Q-values)

**Backward Pass**:
- `compute_td_errors_double_dqn_kernel`: Errores TD con Double DQN
- `compute_grad_W2_kernel`: Gradientes de pesos capa salida
- `compute_grad_b2_kernel`: Gradientes de bias capa salida
- `compute_grad_W1_kernel`: Gradientes de pesos capa oculta
- `compute_grad_b1_kernel`: Gradientes de bias capa oculta
- `apply_gradients_kernel`: Aplicar gradientes con clipping

### Unified Memory

Todos los componentes usan **CUDA Unified Memory** para:
- Eliminar copias explÃ­citas CPUâ†”GPU
- Simplificar gestiÃ³n de memoria
- Optimizar rendimiento en Jetson Xavier (concurrent managed access)

## ğŸ“Š Resultados

### GridWorld 10x10
- **Convergencia**: ~450 episodios
- **Tasa de Ã©xito**: >95% en Ãºltimos 100 episodios
- **Tiempo de entrenamiento**: ~2-3 minutos (Jetson Xavier)

### GridWorld 20x20
- **Convergencia**: ~2000-3000 episodios
- **Tasa de Ã©xito**: >90% en Ãºltimos 100 episodios
- **Tiempo de entrenamiento**: ~15-20 minutos (Jetson Xavier)

### Webots E-puck
- **Convergencia**: Variable segÃºn complejidad del circuito
- **MÃ©trica**: Seguimiento continuo de lÃ­nea por >50 pasos
- **Tiempo de entrenamiento**: ~30-60 minutos

## ğŸ‘¥ Autores

**Proyecto Final de RobÃ³tica**
- Universidad: [Tu Universidad]
- Curso: [Nombre del Curso]
- AÃ±o: 2024

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

## ğŸ™ Agradecimientos

- NVIDIA por CUDA Toolkit y documentaciÃ³n
- Cyberbotics por el simulador Webots
- Comunidad de Deep Reinforcement Learning

## ğŸ“š Referencias

1. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." Nature.
2. Van Hasselt, H., et al. (2016). "Deep Reinforcement Learning with Double Q-learning." AAAI.
3. NVIDIA CUDA Programming Guide
4. Webots Documentation

---

**Â¿Preguntas o problemas?** Abre un issue en el repositorio de GitHub.
